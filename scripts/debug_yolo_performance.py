#!/usr/bin/env python3
"""
YOLO TPU ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‡ãƒãƒƒã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å„å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—ã®æ™‚é–“ã‚’è¨ˆæ¸¬ã—ã¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚’ç‰¹å®š
"""

import sys
import os
import time
import numpy as np
import cv2

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from src.camera import CameraController
from pycoral.utils import edgetpu
from pycoral.adapters import common

def postprocess_yolo_output(output_data, input_shape, conf_threshold=0.5, iou_threshold=0.45):
    """YOLOå½¢å¼ã®å‡ºåŠ›ã‚’å¾Œå‡¦ç†"""
    predictions = output_data[0].transpose()

    boxes = []
    scores = []
    class_ids = []

    h, w = input_shape

    for pred in predictions:
        x_center, y_center, width, height, confidence = pred

        if confidence < conf_threshold:
            continue

        xmin = (x_center - width / 2) / w
        ymin = (y_center - height / 2) / h
        xmax = (x_center + width / 2) / w
        ymax = (y_center + height / 2) / h

        xmin = max(0, min(1, xmin))
        ymin = max(0, min(1, ymin))
        xmax = max(0, min(1, xmax))
        ymax = max(0, min(1, ymax))

        boxes.append([xmin, ymin, xmax, ymax])
        scores.append(float(confidence))
        class_ids.append(0)

    if len(boxes) == 0:
        return []

    boxes_np = np.array(boxes)
    scores_np = np.array(scores)

    boxes_for_nms = boxes_np.copy()
    boxes_for_nms[:, [0, 2]] *= w
    boxes_for_nms[:, [1, 3]] *= h

    indices = cv2.dnn.NMSBoxes(
        boxes_for_nms.tolist(),
        scores_np.tolist(),
        conf_threshold,
        iou_threshold
    )

    detections = []
    if len(indices) > 0:
        for i in indices.flatten():
            detections.append({
                'class': class_ids[i],
                'score': scores[i],
                'bbox': boxes[i]
            })

    return detections


if __name__ == '__main__':
    print("=" * 70)
    print("ğŸ” YOLO TPU ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‡ãƒãƒƒã‚°")
    print("=" * 70)

    # TPUãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    model_path = "models/best_full_integer_quant_edgetpu.tflite"
    print(f"\nğŸ“¦ TPUãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {model_path}")

    interpreter = edgetpu.make_interpreter(model_path)
    interpreter.allocate_tensors()
    print("âœ… Edge TPU ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")

    # ã‚«ãƒ¡ãƒ©åˆæœŸåŒ–
    print("\nğŸ“· ã‚«ãƒ¡ãƒ©ã‚’åˆæœŸåŒ–ä¸­...")
    camera = CameraController(resolution=(640, 480), framerate=30, debug=False)

    if not camera.initialize():
        print("âŒ ã‚«ãƒ¡ãƒ©ã®åˆæœŸåŒ–ã«å¤±æ•—")
        sys.exit(1)

    if not camera.start():
        print("âŒ ã‚«ãƒ¡ãƒ©ã®èµ·å‹•ã«å¤±æ•—")
        camera.cleanup()
        sys.exit(1)

    time.sleep(2)
    print("âœ… ã‚«ãƒ¡ãƒ©åˆæœŸåŒ–å®Œäº†\n")

    # å…¥åŠ›æƒ…å ±å–å¾—
    input_details = interpreter.get_input_details()[0]
    output_details = interpreter.get_output_details()[0]
    input_shape = input_details['shape'][1:3]

    input_scale = input_details['quantization'][0]
    input_zero_point = input_details['quantization'][1]
    output_scale = output_details['quantization'][0]
    output_zero_point = output_details['quantization'][1]

    print(f"å…¥åŠ›ã‚µã‚¤ã‚º: {input_shape}")
    print(f"å…¥åŠ›é‡å­åŒ–: scale={input_scale}, zero_point={input_zero_point}")
    print(f"å‡ºåŠ›é‡å­åŒ–: scale={output_scale}, zero_point={output_zero_point}\n")

    print("=" * 70)
    print("â±ï¸  100ãƒ•ãƒ¬ãƒ¼ãƒ ã®å‡¦ç†æ™‚é–“ã‚’è¨ˆæ¸¬ä¸­...")
    print("=" * 70)

    total_times = {
        'capture': [],
        'resize': [],
        'quantize': [],
        'inference': [],
        'dequantize': [],
        'postprocess': [],
        'total': []
    }

    num_frames = 100

    for i in range(num_frames):
        frame_start = time.time()

        # 1. ãƒ•ãƒ¬ãƒ¼ãƒ å–å¾—
        t1 = time.time()
        frame = camera.capture_frame()
        if frame is None:
            continue
        capture_time = (time.time() - t1) * 1000

        # 2. ãƒªã‚µã‚¤ã‚º
        t2 = time.time()
        resized = cv2.resize(frame, (input_shape[1], input_shape[0]))
        resize_time = (time.time() - t2) * 1000

        # 3. é‡å­åŒ–
        t3 = time.time()
        input_data = (resized.astype(np.float32) / input_scale + input_zero_point).astype(np.int8)
        input_data = np.expand_dims(input_data, axis=0)
        quantize_time = (time.time() - t3) * 1000

        # 4. TPUæ¨è«–
        t4 = time.time()
        interpreter.set_tensor(input_details['index'], input_data)
        interpreter.invoke()
        inference_time = (time.time() - t4) * 1000

        # 5. çµæœå–å¾—ã¨é€†é‡å­åŒ–
        t5 = time.time()
        output_data = interpreter.get_tensor(output_details['index'])
        output_data = (output_data.astype(np.float32) - output_zero_point) * output_scale
        dequantize_time = (time.time() - t5) * 1000

        # 6. YOLOå¾Œå‡¦ç†
        t6 = time.time()
        detections = postprocess_yolo_output(
            output_data,
            input_shape=(input_shape[0], input_shape[1]),
            conf_threshold=0.5,
            iou_threshold=0.45
        )
        postprocess_time = (time.time() - t6) * 1000

        total_time = (time.time() - frame_start) * 1000

        # è¨˜éŒ²
        total_times['capture'].append(capture_time)
        total_times['resize'].append(resize_time)
        total_times['quantize'].append(quantize_time)
        total_times['inference'].append(inference_time)
        total_times['dequantize'].append(dequantize_time)
        total_times['postprocess'].append(postprocess_time)
        total_times['total'].append(total_time)

        if (i + 1) % 20 == 0:
            print(f"  é€²æ—: {i+1}/{num_frames} ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†å®Œäº†")

    # çµ±è¨ˆè¨ˆç®—
    print("\n" + "=" * 70)
    print("ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆï¼ˆå˜ä½: msï¼‰")
    print("=" * 70)

    for key, times in total_times.items():
        avg = np.mean(times)
        std = np.std(times)
        min_t = np.min(times)
        max_t = np.max(times)

        label = {
            'capture': '1. ãƒ•ãƒ¬ãƒ¼ãƒ å–å¾—',
            'resize': '2. ãƒªã‚µã‚¤ã‚º',
            'quantize': '3. é‡å­åŒ–',
            'inference': '4. TPUæ¨è«–',
            'dequantize': '5. é€†é‡å­åŒ–',
            'postprocess': '6. YOLOå¾Œå‡¦ç†',
            'total': 'ã€åˆè¨ˆã€‘'
        }[key]

        print(f"\n{label}:")
        print(f"  å¹³å‡: {avg:6.2f} ms")
        print(f"  æ¨™æº–åå·®: {std:6.2f} ms")
        print(f"  æœ€å°: {min_t:6.2f} ms")
        print(f"  æœ€å¤§: {max_t:6.2f} ms")

    # FPSè¨ˆç®—
    avg_total = np.mean(total_times['total'])
    theoretical_fps = 1000.0 / avg_total

    print("\n" + "=" * 70)
    print(f"ğŸ¯ ç†è«–ä¸Šã®FPS: {theoretical_fps:.1f}")
    print("=" * 70)

    # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æ
    print("\nğŸ“ˆ å‡¦ç†æ™‚é–“ã®å†…è¨³:")
    total_avg = np.mean(total_times['total'])
    for key in ['capture', 'resize', 'quantize', 'inference', 'dequantize', 'postprocess']:
        avg = np.mean(total_times[key])
        percentage = (avg / total_avg) * 100
        label = {
            'capture': 'ãƒ•ãƒ¬ãƒ¼ãƒ å–å¾—',
            'resize': 'ãƒªã‚µã‚¤ã‚º',
            'quantize': 'é‡å­åŒ–',
            'inference': 'TPUæ¨è«–',
            'dequantize': 'é€†é‡å­åŒ–',
            'postprocess': 'YOLOå¾Œå‡¦ç†'
        }[key]
        print(f"  {label:15s}: {avg:6.2f} ms ({percentage:5.1f}%)")

    print("\n" + "=" * 70)

    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    camera.stop()
    camera.cleanup()
    print("\nâœ… ãƒ†ã‚¹ãƒˆå®Œäº†")
